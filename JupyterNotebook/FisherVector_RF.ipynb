{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imagedataset as imgdat\n",
    "import fishervector as fv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Annotations\n",
    "annotations = imgdat.GetAnnotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all sift descriptors in dataset from file...\n",
      "All descriptors loaded ! \n"
     ]
    }
   ],
   "source": [
    "#Try to load all descriptors previously generated if not generate one\n",
    "try:\n",
    "    all_descriptors = imgdat.LoadAllDescriptors()\n",
    "except:\n",
    "    all_descriptors = imgdat.GenerateAllDescriptors(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use standardscaler for all SIFT descriptors before process them with any representation algorithm.\n",
    "sc_dscs = StandardScaler()\n",
    "all_descriptors = sc_dscs.fit_transform(all_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GMM of size 10\n",
      "Initialization 0\n",
      "  Iteration 0\t time lapse 7.98771s\t ll change inf\n",
      "  Iteration 10\t time lapse 11.21843s\t ll change 0.81235\n",
      "  Iteration 20\t time lapse 13.04296s\t ll change 0.31420\n",
      "  Iteration 30\t time lapse 11.96479s\t ll change 0.28584\n",
      "  Iteration 40\t time lapse 13.09017s\t ll change 0.10359\n",
      "  Iteration 50\t time lapse 12.60143s\t ll change 0.03044\n",
      "  Iteration 60\t time lapse 13.20818s\t ll change 0.02088\n",
      "  Iteration 70\t time lapse 11.27446s\t ll change 0.00337\n",
      "  Iteration 80\t time lapse 11.28994s\t ll change 0.00448\n",
      "  Iteration 90\t time lapse 11.37298s\t ll change 0.00148\n",
      "Initialization converged: False\t time lapse 127.14686s\t ll -106.82201\n",
      "GaussianMixture(covariance_type='diag', init_params='kmeans', max_iter=100,\n",
      "        means_init=None, n_components=10, n_init=1, precisions_init=None,\n",
      "        random_state=None, reg_covar=1e-06, tol=0.001, verbose=2,\n",
      "        verbose_interval=10, warm_start=False, weights_init=None)\n",
      "Saving gmm at: means.gmm.npy, covs.gmm.npy, weights.gmm.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\tensorflowproject\\lib\\site-packages\\sklearn\\mixture\\base.py:237: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#Clustering descriptors using KMeans in 120 classes \n",
    "try:\n",
    "    gmm_fv = fv.LoadKmeans()\n",
    "except:\n",
    "    n_clusters = 10\n",
    "    gmm_fv = fv.GenerateGmm(all_descriptors, n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image representation\n",
    "try:\n",
    "    X_fv = np.load(\"fv_X.npy\")\n",
    "    y_fv = np.load(\"fv_y.npy\")\n",
    "except:\n",
    "    X_fv, y_fv = fv.FisherFeatures(annotations, gmm_fv, sc_dscs)\n",
    "    np.save(\"fv_X\", X_fv)\n",
    "    np.save(\"fv_y\", y_fv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification \n",
    "#Dataset splitting into the training set and test set\n",
    "X_train_fv, X_test_fv, y_train_fv, y_test_fv = train_test_split(X_fv, y_fv, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling representation features\n",
    "sc_repr_fv = StandardScaler()\n",
    "X_train_fv = sc_repr_fv.fit_transform(X_train_fv)\n",
    "X_test_fv = sc_repr_fv.transform(X_test_fv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "train score:0.839694656489\n",
      "test score:0.747474747475\n",
      "[[15 23]\n",
      " [ 2 59]]\n"
     ]
    }
   ],
   "source": [
    "#Using A random forest classifier.\n",
    "clf_rf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf_rf.fit(X_train_fv, y_train_fv)\n",
    "\n",
    "print('==========')\n",
    "print('train score:' + str(clf_rf.score(X_train_fv,y_train_fv)))\n",
    "print('test score:' + str(clf_rf.score(X_test_fv,y_test_fv)))\n",
    "y_pred = clf_rf.predict(X_test_fv)\n",
    "y_pred = (y_pred > 0.5)\n",
    "cm = confusion_matrix(y_test_fv, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
